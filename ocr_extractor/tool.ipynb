{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔍 OCR Text Extractor\n",
        "\n",
        "Extract text from images using Optical Character Recognition! Supports:\n",
        "- Single images\n",
        "- Multiple images (creates text files)\n",
        "- ZIP files containing images\n",
        "- Folders with images\n",
        "- **Advanced features:** Language detection, confidence scores, text positioning\n",
        "\n",
        "**Supported formats:** PNG, JPG, JPEG, WEBP, TIFF, BMP\n",
        "**Languages:** English, Vietnamese, Chinese, Japanese, Korean, and more!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 How to Use\n",
        "\n",
        "**Option 1:** Set `path_arg` to your file/folder path\n",
        "**Option 2:** Upload files directly (in Colab)\n",
        "\n",
        "### Examples:\n",
        "```python\n",
        "# Single image\n",
        "path_arg = \"screenshot.png\"\n",
        "\n",
        "# Folder with images  \n",
        "path_arg = \"/path/to/images/\"\n",
        "\n",
        "# ZIP file\n",
        "path_arg = \"documents.zip\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os, zipfile, shutil, uuid, json\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IS_COLAB = True\n",
        "    print(\"🔧 Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"🔧 Running locally\")\n",
        "\n",
        "# Install required packages if needed\n",
        "try:\n",
        "    import pytesseract\n",
        "    from PIL import Image\n",
        "    print(\"✅ Required packages available\")\n",
        "except ImportError:\n",
        "    print(\"📦 Installing required packages...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"pytesseract\", \"opencv-python\", \"Pillow\"])\n",
        "    import pytesseract\n",
        "    from PIL import Image\n",
        "    print(\"✅ Packages installed successfully\")\n",
        "\n",
        "# Try to set tesseract path for different systems\n",
        "try:\n",
        "    # For Windows\n",
        "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "    pytesseract.get_tesseract_version()\n",
        "    print(\"✅ Tesseract found\")\n",
        "except:\n",
        "    try:\n",
        "        # For Linux/Mac\n",
        "        pytesseract.get_tesseract_version()\n",
        "        print(\"✅ Tesseract found\")\n",
        "    except:\n",
        "        print(\"⚠️ Tesseract not found. Please install Tesseract OCR:\")\n",
        "        print(\"   Windows: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
        "        print(\"   Linux: sudo apt-get install tesseract-ocr\")\n",
        "        print(\"   Mac: brew install tesseract\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_images(path):\n",
        "    \"\"\"Get all image files from path (file, folder, or zip)\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"❌ Path not found: {path}\")\n",
        "    \n",
        "    if zipfile.is_zipfile(path):\n",
        "        print(f\"📦 Extracting ZIP: {os.path.basename(path)}\")\n",
        "        base = os.path.splitext(os.path.basename(path))[0]\n",
        "        tmp_dir = f\"ext_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(tmp_dir, exist_ok=True)\n",
        "        zipfile.ZipFile(path).extractall(tmp_dir)\n",
        "        imgs = [os.path.join(r, f) for r, _, fs in os.walk(tmp_dir) \n",
        "                for f in fs if f.lower().endswith(('.png','.jpg','.jpeg','.webp','.tiff','.bmp'))]\n",
        "        return imgs, base, tmp_dir\n",
        "    elif os.path.isdir(path):\n",
        "        print(f\"📁 Scanning folder: {os.path.basename(path)}\")\n",
        "        base = os.path.basename(os.path.normpath(path))\n",
        "        imgs = [os.path.join(r, f) for r, _, fs in os.walk(path) \n",
        "                for f in fs if f.lower().endswith(('.png','.jpg','.jpeg','.webp','.tiff','.bmp'))]\n",
        "        return imgs, base, None\n",
        "    else:\n",
        "        print(f\"🖼️ Processing single image: {os.path.basename(path)}\")\n",
        "        return [path], os.path.splitext(os.path.basename(path))[0], None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"Preprocess image for better OCR results\"\"\"\n",
        "    # Read image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        return None\n",
        "    \n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Apply noise reduction\n",
        "    denoised = cv2.medianBlur(gray, 3)\n",
        "    \n",
        "    # Apply threshold to get binary image\n",
        "    _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    \n",
        "    # Morphological operations to clean up\n",
        "    kernel = np.ones((1,1), np.uint8)\n",
        "    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def extract_text_with_details(image_path, language='eng+vie'):\n",
        "    \"\"\"Extract text with detailed information including confidence scores\"\"\"\n",
        "    try:\n",
        "        # Preprocess image\n",
        "        processed_img = preprocess_image(image_path)\n",
        "        if processed_img is None:\n",
        "            return {\n",
        "                'text': '',\n",
        "                'confidence': 0,\n",
        "                'word_count': 0,\n",
        "                'details': []\n",
        "            }\n",
        "        \n",
        "        # Extract text with confidence scores\n",
        "        data = pytesseract.image_to_data(processed_img, lang=language, output_type=pytesseract.Output.DICT)\n",
        "        \n",
        "        # Extract text\n",
        "        text = pytesseract.image_to_string(processed_img, lang=language)\n",
        "        \n",
        "        # Calculate average confidence\n",
        "        confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]\n",
        "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "        \n",
        "        # Get word details\n",
        "        word_details = []\n",
        "        for i in range(len(data['text'])):\n",
        "            if int(data['conf'][i]) > 0:\n",
        "                word_details.append({\n",
        "                    'text': data['text'][i],\n",
        "                    'confidence': int(data['conf'][i]),\n",
        "                    'bbox': {\n",
        "                        'x': data['left'][i],\n",
        "                        'y': data['top'][i],\n",
        "                        'width': data['width'][i],\n",
        "                        'height': data['height'][i]\n",
        "                    }\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'text': text.strip(),\n",
        "            'confidence': round(avg_confidence, 2),\n",
        "            'word_count': len([w for w in text.split() if w.strip()]),\n",
        "            'details': word_details\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'text': f'Error: {str(e)}',\n",
        "            'confidence': 0,\n",
        "            'word_count': 0,\n",
        "            'details': []\n",
        "        }\n",
        "\n",
        "def detect_language(image_path):\n",
        "    \"\"\"Detect the primary language in the image\"\"\"\n",
        "    try:\n",
        "        processed_img = preprocess_image(image_path)\n",
        "        if processed_img is None:\n",
        "            return 'eng'\n",
        "        \n",
        "        # Try different language combinations\n",
        "        languages = ['eng', 'vie', 'chi_sim', 'jpn', 'kor']\n",
        "        best_lang = 'eng'\n",
        "        best_confidence = 0\n",
        "        \n",
        "        for lang in languages:\n",
        "            try:\n",
        "                data = pytesseract.image_to_data(processed_img, lang=lang, output_type=pytesseract.Output.DICT)\n",
        "                confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]\n",
        "                if confidences:\n",
        "                    avg_conf = sum(confidences) / len(confidences)\n",
        "                    if avg_conf > best_confidence:\n",
        "                        best_confidence = avg_conf\n",
        "                        best_lang = lang\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return best_lang\n",
        "    except:\n",
        "        return 'eng'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process(path, language='auto', include_details=True):\n",
        "    \"\"\"Process images and extract text using OCR\"\"\"\n",
        "    try:\n",
        "        imgs, base, tmp = get_images(path)\n",
        "        if not imgs:\n",
        "            print(\"❌ No valid images found.\")\n",
        "            return\n",
        "\n",
        "        print(f\"📊 Found {len(imgs)} image(s)\")\n",
        "        \n",
        "        out_dir = f\"ocr_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for i, img in enumerate(imgs, 1):\n",
        "            print(f\"🔄 Processing {i}/{len(imgs)}: {os.path.basename(img)}\")\n",
        "            \n",
        "            # Detect language if auto mode\n",
        "            if language == 'auto':\n",
        "                detected_lang = detect_language(img)\n",
        "                print(f\"  🔍 Detected language: {detected_lang}\")\n",
        "            else:\n",
        "                detected_lang = language\n",
        "            \n",
        "            # Extract text\n",
        "            result = extract_text_with_details(img, detected_lang)\n",
        "            \n",
        "            # Save text file\n",
        "            txt_name = os.path.splitext(os.path.basename(img))[0] + \".txt\"\n",
        "            txt_path = os.path.join(out_dir, txt_name)\n",
        "            \n",
        "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"=== OCR Results for {os.path.basename(img)} ===\\n\")\n",
        "                f.write(f\"Language: {detected_lang}\\n\")\n",
        "                f.write(f\"Confidence: {result['confidence']}%\\n\")\n",
        "                f.write(f\"Word Count: {result['word_count']}\\n\")\n",
        "                f.write(f\"\\n--- Extracted Text ---\\n\")\n",
        "                f.write(result['text'])\n",
        "                \n",
        "                if include_details and result['details']:\n",
        "                    f.write(f\"\\n\\n--- Word Details ---\\n\")\n",
        "                    for detail in result['details'][:20]:  # Limit to first 20 words\n",
        "                        f.write(f\"'{detail['text']}' (confidence: {detail['confidence']}%)\\n\")\n",
        "            \n",
        "            # Save detailed JSON if requested\n",
        "            if include_details:\n",
        "                json_name = os.path.splitext(os.path.basename(img))[0] + \".json\"\n",
        "                json_path = os.path.join(out_dir, json_name)\n",
        "                \n",
        "                with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump({\n",
        "                        'filename': os.path.basename(img),\n",
        "                        'language': detected_lang,\n",
        "                        'confidence': result['confidence'],\n",
        "                        'word_count': result['word_count'],\n",
        "                        'text': result['text'],\n",
        "                        'word_details': result['details']\n",
        "                    }, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            results.append({\n",
        "                'filename': os.path.basename(img),\n",
        "                'confidence': result['confidence'],\n",
        "                'word_count': result['word_count'],\n",
        "                'text_preview': result['text'][:100] + '...' if len(result['text']) > 100 else result['text']\n",
        "            })\n",
        "            \n",
        "            print(f\"  ✅ Confidence: {result['confidence']}%, Words: {result['word_count']}\")\n",
        "\n",
        "        # Create summary\n",
        "        summary_path = os.path.join(out_dir, \"summary.txt\")\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=== OCR Processing Summary ===\\n\\n\")\n",
        "            f.write(f\"Total images processed: {len(results)}\\n\")\n",
        "            f.write(f\"Average confidence: {sum(r['confidence'] for r in results) / len(results):.2f}%\\n\")\n",
        "            f.write(f\"Total words extracted: {sum(r['word_count'] for r in results)}\\n\\n\")\n",
        "            \n",
        "            f.write(\"Individual results:\\n\")\n",
        "            for result in results:\n",
        "                f.write(f\"- {result['filename']}: {result['confidence']}% confidence, {result['word_count']} words\\n\")\n",
        "                f.write(f\"  Preview: {result['text_preview']}\\n\\n\")\n",
        "\n",
        "        if len(results) == 1:\n",
        "            # Single file - create individual outputs\n",
        "            txt_files = [f for f in os.listdir(out_dir) if f.endswith('.txt') and f != 'summary.txt']\n",
        "            if txt_files:\n",
        "                final_file = f\"{base}_ocr.txt\"\n",
        "                shutil.move(os.path.join(out_dir, txt_files[0]), final_file)\n",
        "                print(f\"✅ Final output: {final_file}\")\n",
        "        else:\n",
        "            # Multiple files - create ZIP\n",
        "            final_file = f\"{base}_ocr.zip\"\n",
        "            with zipfile.ZipFile(final_file, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "                for root, dirs, files in os.walk(out_dir):\n",
        "                    for file in files:\n",
        "                        z.write(os.path.join(root, file), file)\n",
        "            print(f\"✅ Created ZIP: {final_file} ({len(results)} files)\")\n",
        "\n",
        "        if IS_COLAB:\n",
        "            files.download(final_file)\n",
        "            print(\"📥 Download started!\")\n",
        "        else:\n",
        "            print(f\"📁 Output: {os.path.abspath(final_file)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(out_dir, ignore_errors=True)\n",
        "        if tmp: shutil.rmtree(tmp, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Configuration\n",
        "\n",
        "Set your file path and options here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "path_arg = None  # Set your file/folder path here\n",
        "language = 'auto'  # 'auto', 'eng', 'vie', 'chi_sim', 'jpn', 'kor', or combinations like 'eng+vie'\n",
        "include_details = True  # Set to False to skip detailed JSON output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Run OCR Extraction\n",
        "\n",
        "Execute the OCR process:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if path_arg:\n",
        "    print(f\"🚀 Processing: {path_arg}\")\n",
        "    process(path_arg, language, include_details)\n",
        "elif IS_COLAB:\n",
        "    print(\"📤 Upload your image files...\")\n",
        "    uploaded = files.upload()\n",
        "    for fname in uploaded.keys():\n",
        "        if fname.lower().endswith(('.png','.jpg','.jpeg','.webp','.tiff','.bmp')):\n",
        "            print(f\"🚀 Processing: {fname}\")\n",
        "            process(fname, language, include_details)\n",
        "        else:\n",
        "            print(f\"⚠️ Skipping non-image file: {fname}\")\n",
        "else:\n",
        "    print(\"❗ Please set path_arg or upload in Colab.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
