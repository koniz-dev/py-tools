{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåê Web Scraper & PDF Converter\n",
        "\n",
        "Convert web pages to PDF documents! Features:\n",
        "- **Single page conversion:** Convert individual URLs to PDF\n",
        "- **Batch processing:** Convert multiple URLs from a list\n",
        "- **Website crawling:** Follow links and convert entire sections\n",
        "- **Custom styling:** Apply CSS modifications for better PDF output\n",
        "- **Screenshot mode:** Capture full-page screenshots as PDF\n",
        "- **Content filtering:** Extract specific content before conversion\n",
        "\n",
        "**Advanced features:** Headless browsing, JavaScript rendering, mobile view simulation!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ How to Use\n",
        "\n",
        "**Option 1:** Set `urls` and `mode` for your conversion\n",
        "**Option 2:** Use `crawl_mode` for automatic link discovery\n",
        "\n",
        "### Examples:\n",
        "```python\n",
        "# Single URL\n",
        "urls = [\"https://example.com\"]\n",
        "mode = \"pdf\"\n",
        "\n",
        "# Multiple URLs\n",
        "urls = [\"https://site1.com\", \"https://site2.com\"]\n",
        "mode = \"screenshot\"\n",
        "\n",
        "# Crawl a website\n",
        "crawl_mode = True\n",
        "start_url = \"https://example.com\"\n",
        "max_pages = 10\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os, zipfile, shutil, uuid, json, re, time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IS_COLAB = True\n",
        "    print(\"üîß Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"üîß Running locally\")\n",
        "\n",
        "# Install required packages if needed\n",
        "try:\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.support.ui import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as EC\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    print(\"‚úÖ Required packages available\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing required packages...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"selenium\", \"webdriver-manager\", \"beautifulsoup4\", \"requests\"])\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.support.ui import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as EC\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    print(\"‚úÖ Packages installed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_driver(headless=True, mobile=False):\n",
        "    \"\"\"Setup Chrome WebDriver with appropriate options\"\"\"\n",
        "    chrome_options = Options()\n",
        "    \n",
        "    if headless:\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "    \n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    \n",
        "    if mobile:\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\")\n",
        "        chrome_options.add_argument(\"--window-size=375,667\")\n",
        "    \n",
        "    # PDF generation options\n",
        "    chrome_options.add_argument(\"--disable-web-security\")\n",
        "    chrome_options.add_argument(\"--allow-running-insecure-content\")\n",
        "    \n",
        "    try:\n",
        "        if IS_COLAB:\n",
        "            # In Colab, use system Chrome\n",
        "            driver = webdriver.Chrome(options=chrome_options)\n",
        "        else:\n",
        "            # Local installation with webdriver-manager\n",
        "            driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "        \n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up WebDriver: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_filename(url):\n",
        "    \"\"\"Convert URL to a clean filename\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.replace('www.', '')\n",
        "    path = parsed.path.replace('/', '_').replace('.', '_')\n",
        "    if not path or path == '_':\n",
        "        path = 'index'\n",
        "    \n",
        "    # Remove invalid characters\n",
        "    filename = f\"{domain}_{path}\"\n",
        "    filename = re.sub(r'[<>:\"/\\\\\\\\|?*]', '_', filename)\n",
        "    filename = filename[:100]  # Limit length\n",
        "    \n",
        "    return filename\n",
        "\n",
        "def get_page_info(driver, url):\n",
        "    \"\"\"Get basic information about a web page\"\"\"\n",
        "    try:\n",
        "        title = driver.title\n",
        "        current_url = driver.current_url\n",
        "        \n",
        "        # Get page dimensions\n",
        "        width = driver.execute_script(\"return document.body.scrollWidth\")\n",
        "        height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "        # Get meta information\n",
        "        meta_info = {}\n",
        "        try:\n",
        "            meta_info['description'] = driver.find_element(By.CSS_SELECTOR, 'meta[name=\"description\"]').get_attribute('content')\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "            meta_info['keywords'] = driver.find_element(By.CSS_SELECTOR, 'meta[name=\"keywords\"]').get_attribute('content')\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return {\n",
        "            'title': title,\n",
        "            'url': current_url,\n",
        "            'dimensions': {'width': width, 'height': height},\n",
        "            'meta': meta_info\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def apply_custom_css(driver, css_rules):\n",
        "    \"\"\"Apply custom CSS to improve PDF output\"\"\"\n",
        "    if not css_rules:\n",
        "        return\n",
        "    \n",
        "    css_script = f\"\"\"\n",
        "    var style = document.createElement('style');\n",
        "    style.innerHTML = `{css_rules}`;\n",
        "    document.head.appendChild(style);\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        driver.execute_script(css_script)\n",
        "        time.sleep(1)  # Wait for CSS to apply\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error applying custom CSS: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_pdf(driver, url, output_path, wait_time=3):\n",
        "    \"\"\"Convert a web page to PDF\"\"\"\n",
        "    try:\n",
        "        print(f\"  üåê Loading: {url}\")\n",
        "        driver.get(url)\n",
        "        \n",
        "        # Wait for page to load\n",
        "        time.sleep(wait_time)\n",
        "        \n",
        "        # Wait for any dynamic content\n",
        "        try:\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Get page info\n",
        "        page_info = get_page_info(driver, url)\n",
        "        \n",
        "        # Generate PDF\n",
        "        print(f\"  üìÑ Converting to PDF...\")\n",
        "        pdf_options = {\n",
        "            'paperFormat': 'A4',\n",
        "            'printBackground': True,\n",
        "            'marginTop': 0.5,\n",
        "            'marginBottom': 0.5,\n",
        "            'marginLeft': 0.5,\n",
        "            'marginRight': 0.5\n",
        "        }\n",
        "        \n",
        "        pdf_data = driver.execute_cdp_cmd('Page.printToPDF', pdf_options)\n",
        "        \n",
        "        # Save PDF\n",
        "        import base64\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(base64.b64decode(pdf_data['data']))\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'file': output_path,\n",
        "            'info': page_info\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "def convert_to_screenshot(driver, url, output_path, wait_time=3):\n",
        "    \"\"\"Convert a web page to screenshot PDF\"\"\"\n",
        "    try:\n",
        "        print(f\"  üåê Loading: {url}\")\n",
        "        driver.get(url)\n",
        "        \n",
        "        # Wait for page to load\n",
        "        time.sleep(wait_time)\n",
        "        \n",
        "        # Wait for any dynamic content\n",
        "        try:\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Get page info\n",
        "        page_info = get_page_info(driver, url)\n",
        "        \n",
        "        # Take full page screenshot\n",
        "        print(f\"  üì∏ Taking screenshot...\")\n",
        "        driver.save_screenshot(output_path)\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'file': output_path,\n",
        "            'info': page_info\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "def crawl_website(start_url, max_pages=10, same_domain_only=True):\n",
        "    \"\"\"Crawl a website and discover URLs to convert\"\"\"\n",
        "    try:\n",
        "        print(f\"üï∑Ô∏è Crawling website: {start_url}\")\n",
        "        \n",
        "        visited = set()\n",
        "        to_visit = [start_url]\n",
        "        discovered_urls = []\n",
        "        \n",
        "        base_domain = urlparse(start_url).netloc\n",
        "        \n",
        "        while to_visit and len(discovered_urls) < max_pages:\n",
        "            current_url = to_visit.pop(0)\n",
        "            \n",
        "            if current_url in visited:\n",
        "                continue\n",
        "            \n",
        "            visited.add(current_url)\n",
        "            \n",
        "            try:\n",
        "                response = requests.get(current_url, timeout=10)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                \n",
        "                # Find all links\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link['href']\n",
        "                    full_url = urljoin(current_url, href)\n",
        "                    \n",
        "                    # Filter URLs\n",
        "                    if same_domain_only:\n",
        "                        if urlparse(full_url).netloc != base_domain:\n",
        "                            continue\n",
        "                    \n",
        "                    # Skip non-HTTP URLs and fragments\n",
        "                    if not full_url.startswith(('http://', 'https://')):\n",
        "                        continue\n",
        "                    \n",
        "                    if '#' in full_url:\n",
        "                        full_url = full_url.split('#')[0]\n",
        "                    \n",
        "                    if full_url not in visited and full_url not in to_visit:\n",
        "                        to_visit.append(full_url)\n",
        "                        discovered_urls.append(full_url)\n",
        "                        \n",
        "                        if len(discovered_urls) >= max_pages:\n",
        "                            break\n",
        "                \n",
        "                print(f\"  ‚úÖ Found {len(discovered_urls)} URLs so far...\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö†Ô∏è Error crawling {current_url}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"üï∑Ô∏è Crawling complete. Found {len(discovered_urls)} URLs\")\n",
        "        return discovered_urls[:max_pages]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during crawling: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_urls(urls, mode=\"pdf\", headless=True, mobile=False, custom_css=None, wait_time=3):\n",
        "    \"\"\"Process a list of URLs and convert them\"\"\"\n",
        "    driver = setup_driver(headless, mobile)\n",
        "    if not driver:\n",
        "        print(\"‚ùå Failed to setup WebDriver\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        out_dir = f\"web_conversion_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        \n",
        "        results = []\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        for i, url in enumerate(urls, 1):\n",
        "            print(f\"üîÑ Processing {i}/{len(urls)}: {url}\")\n",
        "            \n",
        "            # Clean filename\n",
        "            filename = clean_filename(url)\n",
        "            \n",
        "            if mode == \"pdf\":\n",
        "                output_path = os.path.join(out_dir, f\"{filename}.pdf\")\n",
        "                result = convert_to_pdf(driver, url, output_path, wait_time)\n",
        "            elif mode == \"screenshot\":\n",
        "                output_path = os.path.join(out_dir, f\"{filename}.png\")\n",
        "                result = convert_to_screenshot(driver, url, output_path, wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Unknown mode: {mode}\")\n",
        "                continue\n",
        "            \n",
        "            # Apply custom CSS if provided\n",
        "            if custom_css and result.get('success'):\n",
        "                apply_custom_css(driver, custom_css)\n",
        "                # Re-convert with custom CSS\n",
        "                if mode == \"pdf\":\n",
        "                    result = convert_to_pdf(driver, url, output_path, wait_time)\n",
        "                elif mode == \"screenshot\":\n",
        "                    result = convert_to_screenshot(driver, url, output_path, wait_time)\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "            if result.get('success'):\n",
        "                successful += 1\n",
        "                print(f\"  ‚úÖ Success: {os.path.basename(result['file'])}\")\n",
        "            else:\n",
        "                failed += 1\n",
        "                print(f\"  ‚ùå Failed: {result.get('error', 'Unknown error')}\")\n",
        "        \n",
        "        # Create summary\n",
        "        summary = {\n",
        "            'total_urls': len(urls),\n",
        "            'successful': successful,\n",
        "            'failed': failed,\n",
        "            'mode': mode,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'results': results\n",
        "        }\n",
        "        \n",
        "        # Save summary\n",
        "        summary_path = os.path.join(out_dir, \"conversion_summary.json\")\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # Create final output\n",
        "        files_in_dir = [f for f in os.listdir(out_dir) if not f.endswith('.json')]\n",
        "        \n",
        "        if len(files_in_dir) == 1:\n",
        "            # Single file\n",
        "            final_file = files_in_dir[0]\n",
        "            shutil.move(os.path.join(out_dir, final_file), final_file)\n",
        "            print(f\"‚úÖ Conversion complete: {final_file}\")\n",
        "        else:\n",
        "            # Multiple files - create ZIP\n",
        "            final_file = f\"web_conversion_{mode}.zip\"\n",
        "            with zipfile.ZipFile(final_file, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "                for root, dirs, files in os.walk(out_dir):\n",
        "                    for file in files:\n",
        "                        z.write(os.path.join(root, file), file)\n",
        "            print(f\"‚úÖ Conversion complete: {final_file} ({len(files_in_dir)} files)\")\n",
        "        \n",
        "        print(f\"üìä Summary: {successful} successful, {failed} failed\")\n",
        "        \n",
        "        if IS_COLAB:\n",
        "            files.download(final_file)\n",
        "            print(\"üì• Download started!\")\n",
        "        else:\n",
        "            print(f\"üìÅ Output: {os.path.abspath(final_file)}\")\n",
        "        \n",
        "        return summary\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during processing: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "        shutil.rmtree(out_dir, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configuration\n",
        "\n",
        "Set your URLs and conversion options here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "urls = [\"https://example.com\"]  # List of URLs to convert\n",
        "mode = \"pdf\"  # \"pdf\" or \"screenshot\"\n",
        "headless = True  # Set to False to see browser window\n",
        "mobile = False  # Set to True for mobile view simulation\n",
        "wait_time = 3  # Seconds to wait for page load\n",
        "\n",
        "# Crawl mode (alternative to manual URL list)\n",
        "crawl_mode = False  # Set to True to crawl a website\n",
        "start_url = \"https://example.com\"  # Starting URL for crawling\n",
        "max_pages = 10  # Maximum pages to crawl\n",
        "same_domain_only = True  # Only crawl same domain\n",
        "\n",
        "# Custom CSS for better PDF output (optional)\n",
        "custom_css = \"\"\"\n",
        "/* Hide navigation and ads for cleaner PDF */\n",
        "nav, .navigation, .navbar, .menu { display: none !important; }\n",
        ".ad, .advertisement, .ads { display: none !important; }\n",
        ".sidebar, .widget { display: none !important; }\n",
        "\n",
        "/* Improve text readability */\n",
        "body { font-size: 12pt !important; line-height: 1.4 !important; }\n",
        "h1, h2, h3 { page-break-after: avoid !important; }\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Run Web Scraping & Conversion\n",
        "\n",
        "Execute the web scraping and conversion process:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if crawl_mode:\n",
        "    print(f\"üï∑Ô∏è Crawl mode enabled\")\n",
        "    print(f\"üöÄ Starting URL: {start_url}\")\n",
        "    print(f\"üìä Max pages: {max_pages}\")\n",
        "    \n",
        "    # Discover URLs by crawling\n",
        "    discovered_urls = crawl_website(start_url, max_pages, same_domain_only)\n",
        "    \n",
        "    if discovered_urls:\n",
        "        print(f\"‚úÖ Found {len(discovered_urls)} URLs to convert\")\n",
        "        process_urls(discovered_urls, mode, headless, mobile, custom_css, wait_time)\n",
        "    else:\n",
        "        print(\"‚ùå No URLs discovered during crawling\")\n",
        "        \n",
        "elif urls and urls != [\"https://example.com\"]:\n",
        "    print(f\"üöÄ Processing {len(urls)} URL(s)\")\n",
        "    print(f\"üîß Mode: {mode}\")\n",
        "    print(f\"üì± Mobile view: {mobile}\")\n",
        "    print(f\"üëª Headless: {headless}\")\n",
        "    \n",
        "    process_urls(urls, mode, headless, mobile, custom_css, wait_time)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùó Please configure URLs or enable crawl mode.\")\n",
        "    print(\"üí° Examples:\")\n",
        "    print(\"   urls = ['https://example.com']\")\n",
        "    print(\"   crawl_mode = True\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
