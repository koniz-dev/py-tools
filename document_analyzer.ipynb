{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Document Analyzer\n",
        "\n",
        "Comprehensive document analysis tool! Features:\n",
        "- **Metadata extraction:** Author, creation date, file size, page count\n",
        "- **Content analysis:** Text statistics, word frequency, readability scores\n",
        "- **Security analysis:** Encryption status, digital signatures, permissions\n",
        "- **Format detection:** File type validation and format-specific analysis\n",
        "- **Batch processing:** Analyze multiple documents at once\n",
        "\n",
        "**Supported formats:** PDF, DOCX, TXT, RTF, HTML, and more!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ How to Use\n",
        "\n",
        "**Option 1:** Set `path_arg` to your file/folder path\n",
        "**Option 2:** Upload files directly (in Colab)\n",
        "\n",
        "### Examples:\n",
        "```python\n",
        "# Single document\n",
        "path_arg = \"document.pdf\"\n",
        "\n",
        "# Folder with documents  \n",
        "path_arg = \"/path/to/documents/\"\n",
        "\n",
        "# ZIP file\n",
        "path_arg = \"documents.zip\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os, zipfile, shutil, uuid, json, re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import textstat\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IS_COLAB = True\n",
        "    print(\"üîß Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"üîß Running locally\")\n",
        "\n",
        "# Install required packages if needed\n",
        "try:\n",
        "    from PyPDF2 import PdfReader\n",
        "    import docx\n",
        "    from PIL import Image\n",
        "    import magic\n",
        "    print(\"‚úÖ Required packages available\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing required packages...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"PyPDF2\", \"python-docx\", \"Pillow\", \"python-magic\", \"textstat\"])\n",
        "    from PyPDF2 import PdfReader\n",
        "    import docx\n",
        "    from PIL import Image\n",
        "    import magic\n",
        "    print(\"‚úÖ Packages installed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_documents(path):\n",
        "    \"\"\"Get all document files from path (file, folder, or zip)\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"‚ùå Path not found: {path}\")\n",
        "    \n",
        "    # Supported document extensions\n",
        "    doc_extensions = ('.pdf', '.docx', '.doc', '.txt', '.rtf', '.html', '.htm', '.md', '.odt')\n",
        "    \n",
        "    if zipfile.is_zipfile(path):\n",
        "        print(f\"üì¶ Extracting ZIP: {os.path.basename(path)}\")\n",
        "        base = os.path.splitext(os.path.basename(path))[0]\n",
        "        tmp_dir = f\"ext_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(tmp_dir, exist_ok=True)\n",
        "        zipfile.ZipFile(path).extractall(tmp_dir)\n",
        "        docs = [os.path.join(r, f) for r, _, fs in os.walk(tmp_dir) \n",
        "                for f in fs if f.lower().endswith(doc_extensions)]\n",
        "        return docs, base, tmp_dir\n",
        "    elif os.path.isdir(path):\n",
        "        print(f\"üìÅ Scanning folder: {os.path.basename(path)}\")\n",
        "        base = os.path.basename(os.path.normpath(path))\n",
        "        docs = [os.path.join(r, f) for r, _, fs in os.walk(path) \n",
        "                for f in fs if f.lower().endswith(doc_extensions)]\n",
        "        return docs, base, None\n",
        "    else:\n",
        "        print(f\"üìÑ Processing single document: {os.path.basename(path)}\")\n",
        "        return [path], os.path.splitext(os.path.basename(path))[0], None\n",
        "\n",
        "def calculate_file_hash(file_path):\n",
        "    \"\"\"Calculate MD5 and SHA256 hashes of a file\"\"\"\n",
        "    md5_hash = hashlib.md5()\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    \n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            md5_hash.update(chunk)\n",
        "            sha256_hash.update(chunk)\n",
        "    \n",
        "    return {\n",
        "        'md5': md5_hash.hexdigest(),\n",
        "        'sha256': sha256_hash.hexdigest()\n",
        "    }\n",
        "\n",
        "def analyze_text_content(text):\n",
        "    \"\"\"Analyze text content for statistics and readability\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return {\n",
        "            'word_count': 0,\n",
        "            'character_count': 0,\n",
        "            'sentence_count': 0,\n",
        "            'paragraph_count': 0,\n",
        "            'readability_score': 0,\n",
        "            'top_words': [],\n",
        "            'language': 'unknown'\n",
        "        }\n",
        "    \n",
        "    # Basic statistics\n",
        "    words = re.findall(r'\\\\b\\\\w+\\\\b', text.lower())\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    paragraphs = [p.strip() for p in text.split('\\\\n\\\\n') if p.strip()]\n",
        "    \n",
        "    # Word frequency\n",
        "    word_freq = Counter(words)\n",
        "    top_words = word_freq.most_common(10)\n",
        "    \n",
        "    # Readability scores\n",
        "    try:\n",
        "        flesch_score = textstat.flesch_reading_ease(text)\n",
        "        flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
        "    except:\n",
        "        flesch_score = 0\n",
        "        flesch_kincaid = 0\n",
        "    \n",
        "    return {\n",
        "        'word_count': len(words),\n",
        "        'character_count': len(text),\n",
        "        'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "        'paragraph_count': len(paragraphs),\n",
        "        'readability_score': flesch_score,\n",
        "        'grade_level': flesch_kincaid,\n",
        "        'top_words': top_words,\n",
        "        'average_word_length': sum(len(word) for word in words) / len(words) if words else 0,\n",
        "        'average_sentence_length': len(words) / len([s for s in sentences if s.strip()]) if sentences else 0\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_pdf(file_path):\n",
        "    \"\"\"Analyze PDF document\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            \n",
        "            # Extract text\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\\\n\"\n",
        "            \n",
        "            # Get metadata\n",
        "            metadata = {}\n",
        "            if reader.metadata:\n",
        "                metadata = {\n",
        "                    'title': str(reader.metadata.get('/Title', '')),\n",
        "                    'author': str(reader.metadata.get('/Author', '')),\n",
        "                    'subject': str(reader.metadata.get('/Subject', '')),\n",
        "                    'creator': str(reader.metadata.get('/Creator', '')),\n",
        "                    'producer': str(reader.metadata.get('/Producer', '')),\n",
        "                    'creation_date': str(reader.metadata.get('/CreationDate', '')),\n",
        "                    'modification_date': str(reader.metadata.get('/ModDate', ''))\n",
        "                }\n",
        "            \n",
        "            # Security analysis\n",
        "            security = {\n",
        "                'encrypted': reader.is_encrypted,\n",
        "                'permissions': {}\n",
        "            }\n",
        "            \n",
        "            if reader.is_encrypted:\n",
        "                try:\n",
        "                    security['permissions'] = {\n",
        "                        'print': reader.get_fields().get('/Print', 'Unknown') if reader.get_fields() else 'Unknown',\n",
        "                        'modify': reader.get_fields().get('/Modify', 'Unknown') if reader.get_fields() else 'Unknown',\n",
        "                        'copy': reader.get_fields().get('/Copy', 'Unknown') if reader.get_fields() else 'Unknown'\n",
        "                    }\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            return {\n",
        "                'type': 'PDF',\n",
        "                'pages': len(reader.pages),\n",
        "                'metadata': metadata,\n",
        "                'security': security,\n",
        "                'text_analysis': analyze_text_content(text)\n",
        "            }\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {'type': 'PDF', 'error': str(e)}\n",
        "\n",
        "def analyze_docx(file_path):\n",
        "    \"\"\"Analyze DOCX document\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        \n",
        "        # Extract text\n",
        "        text = \"\\\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "        \n",
        "        # Get metadata\n",
        "        metadata = {\n",
        "            'title': doc.core_properties.title or '',\n",
        "            'author': doc.core_properties.author or '',\n",
        "            'subject': doc.core_properties.subject or '',\n",
        "            'keywords': doc.core_properties.keywords or '',\n",
        "            'created': str(doc.core_properties.created) if doc.core_properties.created else '',\n",
        "            'modified': str(doc.core_properties.modified) if doc.core_properties.modified else '',\n",
        "            'last_modified_by': doc.core_properties.last_modified_by or ''\n",
        "        }\n",
        "        \n",
        "        # Count elements\n",
        "        element_counts = {\n",
        "            'paragraphs': len(doc.paragraphs),\n",
        "            'tables': len(doc.tables),\n",
        "            'images': len(doc.inline_shapes),\n",
        "            'sections': len(doc.sections)\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'type': 'DOCX',\n",
        "            'metadata': metadata,\n",
        "            'element_counts': element_counts,\n",
        "            'text_analysis': analyze_text_content(text)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'type': 'DOCX', 'error': str(e)}\n",
        "\n",
        "def analyze_text_file(file_path):\n",
        "    \"\"\"Analyze plain text file\"\"\"\n",
        "    try:\n",
        "        # Try different encodings\n",
        "        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "        text = \"\"\n",
        "        encoding_used = \"unknown\"\n",
        "        \n",
        "        for encoding in encodings:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as f:\n",
        "                    text = f.read()\n",
        "                    encoding_used = encoding\n",
        "                    break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "        \n",
        "        if not text:\n",
        "            # If all encodings fail, read as binary and decode with errors='ignore'\n",
        "            with open(file_path, 'rb') as f:\n",
        "                text = f.read().decode('utf-8', errors='ignore')\n",
        "                encoding_used = \"binary_fallback\"\n",
        "        \n",
        "        return {\n",
        "            'type': 'TEXT',\n",
        "            'encoding': encoding_used,\n",
        "            'text_analysis': analyze_text_content(text)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'type': 'TEXT', 'error': str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_document(file_path):\n",
        "    \"\"\"Analyze a single document based on its type\"\"\"\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    file_hash = calculate_file_hash(file_path)\n",
        "    \n",
        "    # Basic file info\n",
        "    basic_info = {\n",
        "        'filename': os.path.basename(file_path),\n",
        "        'file_size': file_size,\n",
        "        'file_size_mb': round(file_size / (1024 * 1024), 2),\n",
        "        'file_extension': file_ext,\n",
        "        'modified_date': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),\n",
        "        'hashes': file_hash\n",
        "    }\n",
        "    \n",
        "    # Analyze based on file type\n",
        "    if file_ext == '.pdf':\n",
        "        analysis = analyze_pdf(file_path)\n",
        "    elif file_ext in ['.docx', '.doc']:\n",
        "        analysis = analyze_docx(file_path)\n",
        "    elif file_ext in ['.txt', '.md', '.rtf']:\n",
        "        analysis = analyze_text_file(file_path)\n",
        "    else:\n",
        "        analysis = {'type': 'UNKNOWN', 'error': f'Unsupported file type: {file_ext}'}\n",
        "    \n",
        "    # Combine basic info with analysis\n",
        "    result = {**basic_info, **analysis}\n",
        "    return result\n",
        "\n",
        "def process(path, include_hashes=True, detailed_analysis=True):\n",
        "    \"\"\"Process documents and generate analysis report\"\"\"\n",
        "    try:\n",
        "        docs, base, tmp = get_documents(path)\n",
        "        if not docs:\n",
        "            print(\"‚ùå No valid documents found.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üìä Found {len(docs)} document(s)\")\n",
        "        \n",
        "        out_dir = f\"analysis_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        \n",
        "        results = []\n",
        "        summary_stats = {\n",
        "            'total_files': len(docs),\n",
        "            'total_size_mb': 0,\n",
        "            'file_types': Counter(),\n",
        "            'total_words': 0,\n",
        "            'total_pages': 0,\n",
        "            'encrypted_files': 0\n",
        "        }\n",
        "        \n",
        "        for i, doc in enumerate(docs, 1):\n",
        "            print(f\"üîÑ Analyzing {i}/{len(docs)}: {os.path.basename(doc)}\")\n",
        "            \n",
        "            try:\n",
        "                result = analyze_document(doc)\n",
        "                results.append(result)\n",
        "                \n",
        "                # Update summary statistics\n",
        "                summary_stats['total_size_mb'] += result.get('file_size_mb', 0)\n",
        "                summary_stats['file_types'][result.get('type', 'UNKNOWN')] += 1\n",
        "                \n",
        "                if 'text_analysis' in result:\n",
        "                    summary_stats['total_words'] += result['text_analysis'].get('word_count', 0)\n",
        "                \n",
        "                if 'pages' in result:\n",
        "                    summary_stats['total_pages'] += result.get('pages', 0)\n",
        "                \n",
        "                if result.get('security', {}).get('encrypted', False):\n",
        "                    summary_stats['encrypted_files'] += 1\n",
        "                \n",
        "                print(f\"  ‚úÖ {result.get('type', 'UNKNOWN')} - {result.get('file_size_mb', 0)} MB\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Error analyzing {os.path.basename(doc)}: {e}\")\n",
        "                results.append({\n",
        "                    'filename': os.path.basename(doc),\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        # Generate detailed report\n",
        "        report_path = os.path.join(out_dir, \"analysis_report.txt\")\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=== DOCUMENT ANALYSIS REPORT ===\\\\n\\\\n\")\n",
        "            f.write(f\"Generated: {datetime.now().isoformat()}\\\\n\")\n",
        "            f.write(f\"Total documents: {summary_stats['total_files']}\\\\n\")\n",
        "            f.write(f\"Total size: {summary_stats['total_size_mb']:.2f} MB\\\\n\")\n",
        "            f.write(f\"Total words: {summary_stats['total_words']:,}\\\\n\")\n",
        "            f.write(f\"Total pages: {summary_stats['total_pages']}\\\\n\")\n",
        "            f.write(f\"Encrypted files: {summary_stats['encrypted_files']}\\\\n\\\\n\")\n",
        "            \n",
        "            f.write(\"File type distribution:\\\\n\")\n",
        "            for file_type, count in summary_stats['file_types'].items():\n",
        "                f.write(f\"  {file_type}: {count}\\\\n\")\n",
        "            \n",
        "            f.write(\"\\\\n=== DETAILED ANALYSIS ===\\\\n\\\\n\")\n",
        "            \n",
        "            for result in results:\n",
        "                f.write(f\"üìÑ {result.get('filename', 'Unknown')}\\\\n\")\n",
        "                f.write(f\"Type: {result.get('type', 'Unknown')}\\\\n\")\n",
        "                f.write(f\"Size: {result.get('file_size_mb', 0)} MB\\\\n\")\n",
        "                \n",
        "                if 'error' in result:\n",
        "                    f.write(f\"Error: {result['error']}\\\\n\")\n",
        "                else:\n",
        "                    if 'metadata' in result and result['metadata']:\n",
        "                        f.write(\"Metadata:\\\\n\")\n",
        "                        for key, value in result['metadata'].items():\n",
        "                            if value:\n",
        "                                f.write(f\"  {key}: {value}\\\\n\")\n",
        "                    \n",
        "                    if 'text_analysis' in result:\n",
        "                        ta = result['text_analysis']\n",
        "                        f.write(f\"Text Analysis:\\\\n\")\n",
        "                        f.write(f\"  Words: {ta.get('word_count', 0):,}\\\\n\")\n",
        "                        f.write(f\"  Characters: {ta.get('character_count', 0):,}\\\\n\")\n",
        "                        f.write(f\"  Sentences: {ta.get('sentence_count', 0)}\\\\n\")\n",
        "                        f.write(f\"  Paragraphs: {ta.get('paragraph_count', 0)}\\\\n\")\n",
        "                        f.write(f\"  Readability Score: {ta.get('readability_score', 0):.1f}\\\\n\")\n",
        "                        f.write(f\"  Grade Level: {ta.get('grade_level', 0):.1f}\\\\n\")\n",
        "                        \n",
        "                        if ta.get('top_words'):\n",
        "                            f.write(f\"  Top Words: {', '.join([f'{word}({count})' for word, count in ta['top_words'][:5]])}\\\\n\")\n",
        "                    \n",
        "                    if 'security' in result and result['security'].get('encrypted'):\n",
        "                        f.write(\"Security: ENCRYPTED\\\\n\")\n",
        "                \n",
        "                f.write(\"\\\\n\" + \"-\"*50 + \"\\\\n\\\\n\")\n",
        "        \n",
        "        # Save JSON report\n",
        "        json_path = os.path.join(out_dir, \"analysis_report.json\")\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'summary': summary_stats,\n",
        "                'documents': results,\n",
        "                'generated': datetime.now().isoformat()\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # Create final output\n",
        "        if len(results) == 1:\n",
        "            final_file = f\"{base}_analysis.txt\"\n",
        "            shutil.move(report_path, final_file)\n",
        "            print(f\"‚úÖ Analysis report: {final_file}\")\n",
        "        else:\n",
        "            final_file = f\"{base}_analysis.zip\"\n",
        "            with zipfile.ZipFile(final_file, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "                for root, dirs, files in os.walk(out_dir):\n",
        "                    for file in files:\n",
        "                        z.write(os.path.join(root, file), file)\n",
        "            print(f\"‚úÖ Analysis package: {final_file}\")\n",
        "        \n",
        "        if IS_COLAB:\n",
        "            files.download(final_file)\n",
        "            print(\"üì• Download started!\")\n",
        "        else:\n",
        "            print(f\"üìÅ Output: {os.path.abspath(final_file)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(out_dir, ignore_errors=True)\n",
        "        if tmp: shutil.rmtree(tmp, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configuration\n",
        "\n",
        "Set your file path and analysis options here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "path_arg = None  # Set your file/folder path here\n",
        "include_hashes = True  # Set to False to skip hash calculation (faster)\n",
        "detailed_analysis = True  # Set to False for basic analysis only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Run Document Analysis\n",
        "\n",
        "Execute the document analysis process:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if path_arg:\n",
        "    print(f\"üöÄ Processing: {path_arg}\")\n",
        "    process(path_arg, include_hashes, detailed_analysis)\n",
        "elif IS_COLAB:\n",
        "    print(\"üì§ Upload your document files...\")\n",
        "    uploaded = files.upload()\n",
        "    doc_files = [fname for fname in uploaded.keys() \n",
        "                 if fname.lower().endswith(('.pdf', '.docx', '.doc', '.txt', '.rtf', '.html', '.htm', '.md'))]\n",
        "    \n",
        "    if doc_files:\n",
        "        print(f\"üöÄ Processing {len(doc_files)} document(s)\")\n",
        "        \n",
        "        # Create temporary directory for uploaded files\n",
        "        temp_dir = f\"temp_{uuid.uuid4().hex[:6]}\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        \n",
        "        for fname in doc_files:\n",
        "            shutil.move(fname, os.path.join(temp_dir, fname))\n",
        "        \n",
        "        process(temp_dir, include_hashes, detailed_analysis)\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "    else:\n",
        "        print(\"‚ùå No supported document files found in upload\")\n",
        "else:\n",
        "    print(\"‚ùó Please set path_arg or upload in Colab.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
